{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from word2vec import * \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "import nltk\n",
    "import gensim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0   352    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  7 2075M    7  151M    0     0  3709k      0  0:09:33  0:00:41  0:08:52 3675k677k      0  0:09:37  0:00:10  0:09:27 3745k    0     0  3769k      0  0:09:23  0:00:22  0:09:01 3889k0  0:09:22  0:00:23  0:08:59 3880kM    0     0  3773k      0  0:09:23  0:00:30  0:08:53 3664k"
     ]
    }
   ],
   "source": [
    "!mkdir GloVe\n",
    "!curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
    "!mkdir fastText\n",
    "!curl -Lo fastText/crawl-300d-2M.vec.zip https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
    "!unzip fastText/crawl-300d-2M.vec.zip -d fastText/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir encoder\n",
    "!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
    "!curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/nehakardam/Documents/UWclasses /EE517 NLP/Project/FacultySupport_SA1_June2_2021.csv')\n",
    "sentences = df[df.SA1.notnull()].SA1.tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent.build_vocab(sentences, tokenize=True)\n",
    "embeddings = infersent.encode(sentences, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent.visualize('Post some examples on the powerpoint. Offer some chances to make up quiz point.', tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "NTHREADS = 8\n",
    "def create_index(a):\n",
    "    index = nmslib.init(space='angulardist')\n",
    "    index.addDataPointBatch(a)\n",
    "    index.createIndex()\n",
    "    return index\n",
    "def get_knns(index, vecs, k):\n",
    "    return zip(*index.knnQueryBatch(vecs, k=k,num_threads=NTHREADS))\n",
    "\n",
    "nn_wvs = create_index(embeddings)\n",
    "to_frame = lambda x: pd.DataFrame(np.array(x)[:,1:])\n",
    "idxs, dists = map(to_frame, get_knns(nn_wvs, embeddings, k=10))\n",
    "df = pd.concat([idxs.stack().to_frame('idx'), dists.stack().to_frame('dist')], axis=1).reset_index().drop('level_1',1).rename(columns={'level_0': 'v1', 'idx': 'v2'})\n",
    "ndf = catted[[\"v1\", \"v2\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters=6).fit(ndf)\n",
    "clustering.labels_\n",
    "plot_scatter(ndf, clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = clustering.labels_\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(X,  color, alpha=0.5):\n",
    "    return plt.scatter(X[:, 0],\n",
    "                       X[:, 1],\n",
    "                       c=color,\n",
    "                       alpha=alpha,\n",
    "                       edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
